
# Serialization/Deserialization

The way this works: you serialize the data from your tables to disk, using the same memory layout that you have in RAM. This avoids having to serialize to JSON or other text-based formats. JSON is slow. Binary data is fast.
So you load your whole database into memory using a single file read from disk, assuming that the mem layout on disk is correct. 

In order to pull this stunt, your deserializer program must know where are the boundaries of the data to be loaded. This is encoded in the header of your database file. The file header contains all of the metadata needed to know how to interpret the file’s bytes, i.e. where things are.

Put simply: the file header is fixed in size, the rest of the file is not. So, the header can be interpreted out of the box, and the data it contains is used to interpret the rest.

# Schema versioning 

One worry arises: what if the schema of the database file changes across software versions?

Let’s start by assuming that the schema of my TreeList _is_ going to change. My program might have new needs, versions might become obsolete, etc.
So we need schema version numbering.

Schema version numbering is done with a non-exhaustive enum:
```zig
pub const Magic = enum(u32) {
    v1 = 0x54524501, // "TREE" + version 1
    v2 = 0x54524502, // "TREE" + version 2
    _,
};
```

## What about byte order? What about memory layouts of the schema versions? 
Zig doesn’t guarantee memory layouts. This means that if the layout of a same struct changes across software versions, the database might read the mem layout wrong, which would lead to incorrect data getting loaded. As a solution, it would be preferable to store versions of the schema using a stable ABI, such as the C ABI. This is done using `extern struct`s.
```zig
   pub const NodeV1 = extern struct {
       id: u64,
       flags: u32,
   };
```

## Byte order could also be a concern. 
You could specify it when reading from file, though I reckon that most consumer hardware runs little endian these days. Still, we’re better off safe than sorry, so let’s be exhaustive:
```zig
const header = Header{
    .version = try reader.readInt(u16, .little),
    .flags = try reader.readInt(u16, .little),
};
```

About storing the schema versions, let’s mark every version change with a major number:
```zig
   // Schema v1
   pub const NodeV1 = extern struct {
       id: u64,
       flags: u32,
   };
   // Schema v2 - Added field at the end
   pub const NodeV2 = extern struct {
       id: u64,
       flags: u32,
       metadata: u32 = 0, 
   };
```

About the versioned structures, we can store versioned meta-structs. This matches the approach I use when instantiating the TreeList itself:
```zig
pub const SchemaV1 = struct {
  pub const v1 = 0x54524501;
  Node = extern struct { /* v1 fields */ };
  Header = extern struct { /* v1 header */ };
};
const TreeList1 = TreeList(SchemaV1);

pub const SchemaV2 = struct {
  pub const v1 = 0x54524502;
  Node = extern struct { /* v2 fields */ };
  Header = extern struct { /* v2 header */ };
};
const TreeList2 = TreeList(SchemaV2);
```

## Version reconciliations

Now we have multiple versions, so we need a way to reconcile them.
Either I have specific loaders for each version:
```zig
pub fn loadTreeList(file: File, path: []const u8) !void {
    const header = try file.reader().readStruct(TreeListFileHeader);

    switch (header.magic) {
        .v1 => try loadTreeListV1(tree_list, file, header),
        .v2 => try loadTreeListV2(tree_list, file, header),
        else => return error.UnsupportedVersion,
    }
}
```

Or I can migrate versions in place:
```zig
pub fn loadTreeList(file: File, path: []const u8) !void {
    const header = try file.reader().readStruct(TreeListFileHeader);

    switch (header.magic) {
        .v1 => { 
          const result = try migrateV1ToV2(file, header); 
          try loadTreeListV2(result.file, result.header),
        },
        .v2 => try loadTreeListV2(tree_list, allocator, file, header),
        else => return error.UnsupportedVersion,
    }
}
```

# backwards compat’ when _writing_

And then we have other problems: I can maintain backwards compatibility when _reading_, but am I expected to also maintain it when _writing_ ?

What if the user is using an older schema ? More recent versions of the program would have to serialize to the newer schemas, unless we also wanted to maintain backwards compatibility on the user’s system: we’d have to convert the new schema to the older version, just to write to disk. This sounds to me like it would be really difficult to maintain. 

Let’s assume that my program only wants to write in the latest format: reads can be backwards compatible, writes can not. (There are ways to make the format _forwards compatible_ too, which we’ll discuss below.)

Also, the contents of the file header might change depending on the schema version. This means that the magic/version number should always be the first in the file: it needs to be read first - to identify which schema is needed to interpret the header.

This potentially simplifies the process though, as it allows each TreeList to implement its own de-serialization logic:

```zig
const T1 = TreeList(Schema1);
const T2 = TreeList(Schema2);

pub fn loadTreeList(path: []const u8) !T2 {
    var file = try std.fs.cwd().openFile(path, .{});
    defer file.close();

    const reader = try file.reader();
    const magic = try reader.readInt(u16, .little)

    return switch (@intFromEnum(Version, magic)) {
        .v1 => { 
          const treelist_v1 = try T1.readDb(file);
          T1toT2(treelist_v1); 
        },
        .v2 => try T2.readDb(file),
        else => return error.UnsupportedVersion,
    }
}
```

## What’s the difference between magic number and version number?

From what I understand, the magic number is often used to identify the file signature, i.e. the file type. Version numbers generally follow it in place, but they’re considered separate things. The magic number/magic bytes are meant to avoid confusion when trying to read a file which might have the correct extension but the wrong file signature.
One could encode the two together:
```zig
pub const Magic = enum(u32) {
    v1 = 0x54524501, // "TREE" + version 1
    v2 = 0x54524502, // "TREE" + version 2
    _,
};
```

## Forwards compatibility?

For backwards compat, I picture it as: store your schemas in source code, pack the version number in the binary file header. Upon opening the file, read the version number first, and figure out which schema to use from there. Newer versions of the software can have a mapper to transform older schemas into their newer counterparts.

How does one do forwards compatibility, though ? 

Cfillion’s answer:
> In general: by storing the size(s) of the structure(s), so older code can ignore fields added later that it doesn't know about.
> This assumes that newer fields are always going to be _appended_ into your structs. That becomes a requirement: never touch pre-existing fields. That’s the same as with text, though… you couldn't change the existing format later, only add to it.

This requires commitments to the schemas, commitments which I will only want to make once I reach a release version - or during a hardening phase.
